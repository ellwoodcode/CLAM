[
  {
    "idea": "\nTitle: Self-Supervised Pretraining with Domain-Adaptive Fine-Tuning\n\nJustification: Labeled data is often scarce in weakly supervised settings, and domain shifts (e.g., different scanners, staining protocols) can degrade performance. Pretraining models using self-supervised objectives (e.g., contrastive learning) on large, unlabeled WSI datasets can provide robust feature representations. Subsequently, adapting the model to the target domain with a small amount of labeled data (domain-adaptive fine-tuning) helps bridge distribution gaps, thus improving generalisation.\n\nInspired by: Papers demonstrating the effectiveness of self-supervised learning for medical imaging, particularly in scenarios with limited annotations, and showing improved generalisation after domain-adaptive fine-tuning.\n",
    "scores": {
      "novelty": 4,
      "potential": 8,
      "feasibility": 9,
      "commentary": "Self-supervised pretraining and domain-adaptive fine-tuning have been explored in WSI and MIL literature, so novelty is moderate. The approach has strong empirical support for improving generalisability, especially across domains. Implementation in a CLAM pipeline is straightforward, as feature extractors can be pretrained and fine-tuned with modest code changes.",
      "total": 21
    }
  },
  {
    "idea": "\nTitle: Curriculum Learning with Progressive Label Refinement\n\nJustification: Weakly supervised models often struggle due to noisy or coarse labels. Curriculum learning, wherein the model starts training on easier or more reliable instances before incorporating harder or noisier data, can guide the model toward more generalisable representations. Coupling this with progressive label refinement\u2014where the model iteratively updates pseudo-labels or confidence scores\u2014can further improve learning from weak supervision.\n\nInspired by: Recent studies in weakly supervised learning that highlight the benefits of structured learning schedules and iterative pseudo-labeling for enhanced model generalisation.\n",
    "scores": {
      "novelty": 7,
      "potential": 8,
      "feasibility": 7,
      "commentary": "Curriculum learning and label refinement have seen limited but growing application in WSI/MIL; combining them is relatively novel. The approach is promising for generalisation, especially in heterogeneous datasets. Implementation is feasible but would require careful integration with attention and pseudo-labeling mechanisms.",
      "total": 22
    }
  },
  {
    "idea": "\nTitle: Self-Supervised Pretraining for Feature Robustness\n\nJustification: Pretraining models on large, unlabeled datasets using self-supervised objectives (such as contrastive learning or masked image modeling) can help networks learn more general, transferable features. These features are less likely to overfit to spurious correlations present in weakly annotated or limited labeled data, thus improving generalisation, especially in the context of whole slide images (WSI) where annotation is costly and sparse.\n\nInspired by: Abstracts discussing the effectiveness of self-supervised learning and transfer learning approaches in medical imaging and WSI tasks.\n",
    "scores": {
      "novelty": 4,
      "potential": 8,
      "feasibility": 9,
      "commentary": "Self-supervised pretraining has already been explored in WSI and MIL literature, so novelty is moderate. However, its potential for improving generalisation is high, and integration into a CLAM pipeline is straightforward using pretrained encoders.",
      "total": 21
    }
  },
  {
    "idea": "\nTitle: Multi-Scale Attention-Based Aggregation\n\nJustification: Incorporating multi-scale attention mechanisms allows models to focus adaptively on relevant tissue structures at different resolutions, which is crucial in WSI analysis. By aggregating features across scales using attention, the model can generalise better to variations in stain, magnification, and tissue morphology, thus overcoming the limitations of weak supervision.\n\nInspired by: Abstracts highlighting advances in attention-based and multi-scale feature aggregation methods for WSI classification and generalisation.\n",
    "scores": {
      "novelty": 5,
      "potential": 8,
      "feasibility": 8,
      "commentary": "Multi-scale attention has been explored in WSI and MIL literature, but specific implementations vary; incremental novelty. Strong potential for generalisation due to explicit multi-resolution modeling. Feasible to integrate into CLAM with moderate code changes, leveraging existing attention and feature aggregation modules.",
      "total": 21
    }
  },
  {
    "idea": "\nTitle: Domain-Adaptive Mixup Augmentation\n\nJustification: Traditional data augmentations may not capture the complex variability in histopathology images. Domain-adaptive mixup, where virtual samples are generated by interpolating not only between images but also between domains (e.g., different scanners or institutions), can help the model learn domain-invariant features. This leads to improved robustness and generalisation to unseen data distributions.\n\nInspired by: Abstracts referencing domain adaptation, distributional robustness, and mixup-inspired augmentation techniques in weakly supervised learning.\n",
    "scores": {
      "novelty": 7,
      "potential": 8,
      "feasibility": 8,
      "commentary": "Mixup and domain adaptation have been explored separately in WSI/MIL, but explicit domain-adaptive mixup is less common and promising for generalisation. Implementation is feasible in CLAM with moderate code changes to the data pipeline.",
      "total": 23
    }
  }
]